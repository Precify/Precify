{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXB4SlvrYYyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive \n",
        "#drive.mount('/content/gdrive') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARN7a36TYYwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTVb7JlyYsf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "server = \"stark-anchorage-45962.herokuapp.com\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKIdA1YUdf8f",
        "colab_type": "code",
        "outputId": "4922d501-d021-472a-e0b6-d8bf393ac479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ67auEnHLg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9a26e39d-3b3d-47c5-b109-21fbb892471d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLu_GXPEUuYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "from urllib.parse import quote\n",
        "\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "import http.client\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F108LPx-dafp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_lst_all = [\"https://economictimes.indiatimes.com/news/politics-and-nation/coronavirus-cases-in-india-live-news-latest-updates-april8/liveblog/75038326.cms\", \"https://www.pharmaceutical-technology.com/special-focus/covid-19/coronavirus-covid-19-outbreak-latest-information-news-and-updates/\", \"https://www.business-standard.com/article/current-affairs/coronavirus-live-updates-covid-19-cases-in-india-global-death-toll-state-wise-delhi-maharasthra-tablighi-nizamuddin-lockdown-extension-latest-news-120040800236_1.html\", \"https://timesofindia.indiatimes.com/india/coronavirus-india-cases-live-news-updates-lockdown-in-india-likely-to-be-extended/liveblog/75074630.cms\"]\n",
        "\n",
        "url_lst_statewise = [\"https://timesofindia.indiatimes.com/india\"]\n",
        "\n",
        "url_lst_citywise = [\"https://timesofindia.indiatimes.com/city\"]\n",
        "\n",
        "keywords = [\"corona\", \"coronavirus\", \"lockdown\", \"covid-19\", \"covid19\", \"masks\", \"handwash\", \"mask\", \"infection\", \"staysafe\", \"quarantine\", \"stayathome\", \"pandemic\", \"shutdown\"]\n",
        "\n",
        "apikey = \"c8c54678d99e43d9b11a09d74e8dfa51\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZC6VPDzlVAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connection(conn, link, author, source, text, location) :\n",
        "    headers = {'Content-type': 'application/json'}\n",
        "    data = {'url': str(link), 'author' : str(author), 'source' : str(source), 'text' : str(text), 'location' : str(location)}\n",
        "    json_data = json.dumps(data)\n",
        "    conn.request('POST', '/addPost', json_data, headers)\n",
        "    response = conn.getresponse()\n",
        "    print(response.read().decode())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUIsT6l1lf9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def allNews() :\n",
        "    articles = []\n",
        "    urls = []\n",
        "    #response = requests.get(url_lst_all[0], allow_redirects = True)\n",
        "    #soup = BeautifulSoup(response.text)\n",
        "    '''r = requests.get(url_lst_all[0]) \n",
        "    soup = BeautifulSoup(r.content, 'html5lib') \n",
        "    #text1 = soup.findAll('h3', class_ = \"\")\n",
        "    text2 = soup.findAll('div', class_ = \"blogSysn\")\n",
        "    #text3 = soup.findAll('span')\n",
        "    for text in text2 :\n",
        "        if(len(text.text) > 50):\n",
        "            articles.append(text.text)\n",
        "            \n",
        "    r = requests.get(url_lst_all[1]) \n",
        "    soup = BeautifulSoup(r.content, 'html5lib') \n",
        "    #text1 = soup.findAll('h3')\n",
        "    text2 = soup.findAll('a')\n",
        "    text3 = soup.findAll('li', class_ = \"row\")\n",
        "    #text4 = soup.findAll('div', class_ = \"large-9 columns\")\n",
        "    for txt in text3:\n",
        "        regex = re.compile(r'[\\n\\r\\t]')\n",
        "        s = regex.sub(\" \", txt.text)\n",
        "        articles.append(s)\n",
        "    for text in text2 :\n",
        "        flag = 0\n",
        "        blog = \"\"\n",
        "        for ch in text:\n",
        "            if(isinstance(ch, bs4.element.NavigableString)) :\n",
        "                blog = blog + ch\n",
        "        if(len(blog) > 25) :\n",
        "            articles.append(blog)'''\n",
        "        \n",
        "\n",
        "    r = requests.get(url_lst_all[2]) \n",
        "    soup = BeautifulSoup(r.content, 'html5lib') \n",
        "    tags = soup.findAll('a')\n",
        "    i = 0\n",
        "    for tag in tags:\n",
        "        try :\n",
        "            if(tag['target'] == \"_blank\") :\n",
        "                if(i < 3) :\n",
        "                    i = i + 1\n",
        "                    continue\n",
        "                else :\n",
        "                    if(len(tag.text) > 50) :\n",
        "                        my_str = tag.text\n",
        "                        _RE_COMBINE_WHITESPACE = re.compile(r\"(?a:\\s+)\")\n",
        "                        _RE_STRIP_WHITESPACE = re.compile(r\"(?a:^\\s+|\\s+$)\")\n",
        "\n",
        "                        my_str = _RE_COMBINE_WHITESPACE.sub(\" \", my_str)\n",
        "                        my_str = _RE_STRIP_WHITESPACE.sub(\"\", my_str)\n",
        "                        for word in keywords:\n",
        "                          if word in my_str.lower():\n",
        "                            articles.append(my_str)\n",
        "                            g = \"https://www.business-standard.com\" + tag['href']\n",
        "                            urls.append(g)\n",
        "                            break\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "    url = ('http://newsapi.org/v2/top-headlines?'\n",
        "       'country=in&'\n",
        "       'apiKey=c8c54678d99e43d9b11a09d74e8dfa51')\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    for article in data['articles'] :\n",
        "        for word in keywords:\n",
        "            try :\n",
        "                if word in article['content'].lower():\n",
        "                    articles.append(article['description'])\n",
        "                    urls.append(article['url'])\n",
        "            except :\n",
        "                continue\n",
        "    \n",
        "    r = requests.get(url_lst_all[3]) \n",
        "    soup = BeautifulSoup(r.text, 'html5lib') \n",
        "    tags = soup.findAll('script')\n",
        "    data = json.loads(tags[1].text, strict=False)\n",
        "    for t in data['liveBlogUpdate'] :\n",
        "        for word in keywords:\n",
        "            if word in t['headline'].lower() :\n",
        "                articles.append(t['headline'])\n",
        "                urls.append(t['url'])\n",
        "                break\n",
        "    \n",
        "    return articles, urls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1rA3im8ljlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def citywise() :\n",
        "    prefix = \"https://timesofindia.indiatimes.com\"\n",
        "    covid_links = []\n",
        "    #covid_citywise = []\n",
        "    response = requests.get(url_lst_citywise[0], allow_redirects = True)\n",
        "    soup = BeautifulSoup(response.text)\n",
        "    one_a_tag = soup.findAll('a')\n",
        "    #print(one_a_tag)\n",
        "    no_cities = 67\n",
        "    index = 0\n",
        "    cities_lst = []\n",
        "    for line in one_a_tag :\n",
        "            if(index > no_cities) :\n",
        "                break\n",
        "            try:\n",
        "                link = line['href']\n",
        "                if(link[:6] == \"/city/\") :\n",
        "                    cities_lst.append(link)\n",
        "                    index = index + 1\n",
        "            except:\n",
        "                continue\n",
        "    #print(cities_lst)\n",
        "    articles_citywise = []\n",
        "    i = 0\n",
        "    for url in cities_lst:\n",
        "        covid_citywise = []\n",
        "        articles_lst = []\n",
        "        url_ = prefix + url\n",
        "        #print(url_)\n",
        "        words = url_.split(\"/\")\n",
        "        #print(words[-1])\n",
        "        #covid_citywise.append(words[-1])\n",
        "        articles_lst.append(words[-1])\n",
        "        length_ = len(url_)\n",
        "        length = len(url)\n",
        "        #print(url_)\n",
        "        response = requests.get(url_, allow_redirects = True)\n",
        "        soup = BeautifulSoup(response.text)\n",
        "        one_a_tag = soup.findAll('a')\n",
        "        for link in one_a_tag :\n",
        "            try :\n",
        "                if(link['href'][-4:] == \".cms\") :\n",
        "                    if(link['href'][:length] == url):\n",
        "                        temp = prefix + link['href']\n",
        "                        articles_lst.append(temp)\n",
        "                    elif(link['href'][:length_] == url_):\n",
        "                        articles_lst.append(link['href'])\n",
        "            except :\n",
        "                continue\n",
        "        #for ele in articles_lst :\n",
        "            #print(ele)\n",
        "        #break\n",
        "        articles_citywise.append(articles_lst)\n",
        "        #i = i + 1\n",
        "        covid_links = []\n",
        "        locations = []\n",
        "        title = []\n",
        "        content = []\n",
        "        urls = []\n",
        "        authors = []\n",
        "        sources = []\n",
        "    #print(articles_citywise)\n",
        "    #return\n",
        "    #from newspaper import Article\n",
        "    for articles_lst in articles_citywise :\n",
        "        covid_citywise = []\n",
        "        i = 0\n",
        "        for link in articles_lst:\n",
        "            #print(link)\n",
        "            if(i == 0) :\n",
        "                loc = link\n",
        "                print(loc)\n",
        "                i = i + 1\n",
        "                continue\n",
        "            try :\n",
        "                article = Article(link)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                article.nlp()\n",
        "                for word in keywords:\n",
        "                    if word in article.text.lower() :\n",
        "                        locations.append(loc)\n",
        "                        title.append(article.title)\n",
        "                        content.append(article.summary)\n",
        "                        urls.append(link)\n",
        "                        if(len(article.authors) != 0):\n",
        "                          authors.append(article.authors)\n",
        "                        else:\n",
        "                          authors.append(\"\")\n",
        "                        sources.append(article.source_url)\n",
        "                        #print(link)\n",
        "                        #print(article.title)\n",
        "                        #print(article.summary)\n",
        "                        break\n",
        "            except :\n",
        "                continue\n",
        "    \n",
        "        #break\n",
        "        #covid_links.append(covid_citywise)\n",
        "    data = dict()\n",
        "    data[\"title\"] = title\n",
        "    data[\"content\"] = content\n",
        "    data[\"location\"] = locations\n",
        "    data['url'] = urls\n",
        "    data['author'] = authors\n",
        "    data['source'] = sources\n",
        "    #with open(\"./data_scrapper/citywise.json\", 'w') as fh:\n",
        "    #    fh.write(json.dumps(data))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY2IgiL7lmcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def statewise() :\n",
        "    prefix = \"https://timesofindia.indiatimes.com\"\n",
        "    #covid_links = []\n",
        "    response = requests.get(url_lst_statewise[0], allow_redirects = True)\n",
        "    soup = BeautifulSoup(response.text)\n",
        "    data = soup.findAll('meta')\n",
        "    links_lst =[]\n",
        "    i = -1\n",
        "    for one in data :\n",
        "        try:\n",
        "            if(one['itemprop'] == \"url\") :\n",
        "                url = str(one['content']).split('/')\n",
        "                if(url[-2] == \"india\") :\n",
        "                    i = i + 1\n",
        "                    if(i == 0) :\n",
        "                        continue\n",
        "                    else :\n",
        "                        links_lst.append(str(one['content']))\n",
        "        except:\n",
        "            continue\n",
        "    #print(links_lst)\n",
        "    locations = []\n",
        "    title = []\n",
        "    content = []\n",
        "    urls = []\n",
        "    authors = []\n",
        "    sources = []\n",
        "    for link in links_lst :\n",
        "        #print(link)\n",
        "        #covid_statewise = []\n",
        "        parser = link.split(\"/\")\n",
        "        #covid_statewise.append(parser[-1])\n",
        "        loc = parser[-1]\n",
        "        r = requests.get(link) \n",
        "        print(loc)\n",
        "        soup = BeautifulSoup(r.content, 'html5lib') \n",
        "        #tags = soup.findAll('script')\n",
        "        tags_ = soup.findAll('a')\n",
        "        #i = 0\n",
        "        for tag in tags_ :\n",
        "            try :\n",
        "                if(tag[\"hid\"] is not None) :\n",
        "                    #print(tag)\n",
        "                    article = Article(prefix + tag['href'])\n",
        "                    article.download()\n",
        "                    article.parse()\n",
        "                    article.nlp()\n",
        "                    for word in keywords:\n",
        "                        if word in article.text.lower() :\n",
        "                            locations.append(loc)\n",
        "                            title.append(article.title)\n",
        "                            content.append(article.summary)\n",
        "                            urls.append(prefix + tag['href'])\n",
        "                            if(len(article.authors) != 0):\n",
        "                              authors.append(article.authors)\n",
        "                            else:\n",
        "                              authors.append(\"\")\n",
        "                            sources.append(article.source_url)\n",
        "                            #print(article.title)\n",
        "                            #print(article.summary)\n",
        "                            #print(\"yes\")\n",
        "                            break\n",
        "                    \n",
        "            except:\n",
        "                continue\n",
        "        #break          \n",
        "    data = dict()\n",
        "    data[\"title\"] = title\n",
        "    data[\"content\"] = content\n",
        "    data[\"location\"] = locations\n",
        "    data['url'] = urls\n",
        "    data['author'] = authors\n",
        "    data['source'] = sources\n",
        "    #print(data)\n",
        "    #with open(\"./data_scrapper/statewise.json\", 'w') as fh:\n",
        "    #    fh.write(json.dumps(data))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7do1i6tlprY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all india news\n",
        "def get_all_news() :\n",
        "  articles, urls = allNews()\n",
        "  conn = http.client.HTTPSConnection(server)\n",
        "  #limit = 0\n",
        "  i = 0\n",
        "  for article in articles:\n",
        "      connection(conn, urls[i], \"\", \"\", article, \"india\")\n",
        "      i = i + 1\n",
        "      #limit = limit + 1\n",
        "      #if(limit == 20) :\n",
        "      #    time.sleep(61)\n",
        "      #    limit = 0\n",
        "      #    conn = http.client.HTTPSConnection(server)\n",
        "      #print(article)\n",
        "      #print(\"\")\n",
        "      #break\n",
        "  #convert set of strings into json\n",
        "  data = dict()\n",
        "  data['articles'] = articles\n",
        "  data['url'] = urls\n",
        "  #with open(\"./data_scrapper/all.json\", 'w') as fh:\n",
        "  #    fh.write(json.dumps(data))\n",
        "  return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJXxi4Wxl1ZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#statewise\n",
        "def get_statewise_news() :\n",
        "  data = statewise()\n",
        "  #title = data[\"title\"] \n",
        "  content = data[\"content\"] \n",
        "  locations = data[\"location\"]  \n",
        "  urls = data['url'] \n",
        "  authors = data['author']  \n",
        "  sources = data['source'] \n",
        "  l = len(urls)\n",
        "  #limit = 0\n",
        "  #print(data)\n",
        "  print(l)\n",
        "  conn = http.client.HTTPSConnection(server)\n",
        "  for j in range(l) :\n",
        "    connection(conn, urls[j], authors[j], sources[j], content[j], locations[j])\n",
        "    #limit = limit + 1\n",
        "    #if(limit == 20) :\n",
        "    #    time.sleep(61)\n",
        "    #    limit = 0\n",
        "    #    conn = http.client.HTTPSConnection(server)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aat9AdzOl1W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#citywise\n",
        "def get_citywise_news() :\n",
        "  data = citywise()\n",
        "  #title = data[\"title\"] \n",
        "  content = data[\"content\"] \n",
        "  locations = data[\"location\"]  \n",
        "  urls = data['url'] \n",
        "  authors = data['author']  \n",
        "  sources = data['source'] \n",
        "  l = len(urls)\n",
        "  #limit = 0\n",
        "  conn = http.client.HTTPSConnection(server)\n",
        "  for j in range(l) :\n",
        "    connection(conn, urls[j], authors[j], sources[j], content[j], locations[j])\n",
        "    #limit = limit + 1\n",
        "    #if(limit == 20) :\n",
        "    #    time.sleep(61)\n",
        "    #    limit = 0\n",
        "    #   conn = http.client.HTTPSConnection(server)\n",
        "  return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kimkxdfZo6Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run() :\n",
        "  get_all_news()\n",
        "  get_statewise_news()\n",
        "  get_citywise_news()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRAeyzITIYpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while(1) :\n",
        "  run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}